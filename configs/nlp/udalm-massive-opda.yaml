dataset:
  name: massive # choices are ['amazon', 'clinc', 'massive']
  # root_path: /home/heyjoonkim/Universal-Domain-Adaptation/data/ # /path/to/dataset/root
  root_path: /home/heyjoonkim/data/datasets/ # /path/to/dataset/root# total 2 class, 2 class common -> CDA setting
  num_common_class: 8
  num_source_class: 13


model:
  model_name_or_path: bert-base-uncased

train:
  # train: False              # train model <-> only test model
  train: True                 # train model <-> only test model
  max_length: 512             # max length for tokenizer
  batch_size: 64         
  num_train_epochs: 10        # total training epoch 
  lr: 0.01 
  mlm_probability: 0.15       # for mlm pre-training
  mlm_weight: 0.88            # weight for mlm training
  lr_scheduler_type: linear   # scheduler type
  early_stop: 3               # early stopping epoch
  seed: 1234                  # random seed

test:
  min_threshold: 0.0
  max_threshold: 1.0
  step: 0.005
  threshold: 0.5
  batch_size: 64
  fpr_rate: 0.95

log:
  output_dir: /home/heyjoonkim/data/universal_domain_adaptation/