data:
  dataset:
    name: office # choices are ['office', 'officehome', 'caltech-imagenet', 'visda2017']
    # root_path: /home/heyjoonkim/Universal-Domain-Adaptation/data/office # /path/to/dataset/root
    root_path: /home/heyjoonkim/data/datasets/office # /path/to/dataset/root
    # office : amazon (0), dslr (1), webcam (2)
    source: 2 # source domain index
    target: 1 # target domain index
    n_share: 10 # number of classes to be shared
    n_source_private: 10 # number of classes in source private domain
    n_total: 31 # number of classes in total

  dataloader:
    class_balance: true #
    data_workers: 3 # how many workers to use for train dataloaders
    # batch_size: 36 # batch_size for source domain and target domain respectively
    batch_size: 4 # batch_size for source domain and target domain respectively

model:
  base_model: resnet50 # choices=['resnet50', 'vgg16']
  # pretrained_model: /workspace/fubo/resnet50.pth # /path/to/pretrained/model

train:
  # 10000 step from the original paper with batch size 36
  # we used batch size of 4 (due to GPU size), so we increased the total step to 10000 * 9 = 90000
  # min_step: 10000 # minimum steps to run. run epochs until it exceeds the minStep
  min_step: 90000 # minimum steps to run. run epochs until it exceeds the minStep
  lr: 0.001 # learning rate for new layers. learning rate for finetune is 1/10 of lr
  weight_decay: 0.0005
  sgd_momentum: 0.9
  early_stop: 50   # early stopping epoch
  K: 50
  gamma: 0.7
  mu: 0.7
  temp: 0.1
  lam: 0.1
  MQ_size: 2000

test:
  test_only: False # test a given model and exit
  threshold: 0.5 # hyper-parameter w

misc:
  gpus: 1 # how many GPUs to be used, 0 indicates CPU only

log:
  root_dir: log # the log directory (log directory will be {root_dir}/{method}/time/)